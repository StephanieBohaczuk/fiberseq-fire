import pandas as pd
import sys
import os

ref = config["ref"]
ref_name = config["ref_name"]
n_chunks = config["n_chunks"]
chunks = [f"{i+1}-of-{n_chunks}" for i in range(n_chunks)]
excludes = config["excludes"]
dhs = config["dhs"]
max_t = config.get("max_t", 4)
max_bins = config.get("bins", 75)
keep_chrs = config.get("keep_chromosomes", ".*")


fai = pd.read_csv(f"{ref}.fai", sep="\t", names=["chr", "length", "x", "y", "z"])
chroms = sorted([chrom for chrom in fai["chr"] if "_" not in chrom])
genome_len = fai["length"].sum()
window_size = int(genome_len / n_chunks) + 1
conda = "fiberseq-smk"
bins = list(range(1, max_bins + 1))
data = pd.read_csv(config["manifest"], sep="\t", comment="#").set_index("sample")

print(data.index)

types = ["fdr", "acc", "link", "nuc"]
types_to_col = {"fdr": 4, "acc": 5, "link": 6, "nuc": 7}


def get_mem_mb(wildcards, attempt):
    if attempt < 3:
        return attempt * 1024 * 32
    return attempt * 1024 * 48


wildcard_constraints:
    chrom="|".join(chroms),
    call="|".join(["msp", "m6a"]),
    chunk="|".join(chunks),
    sm="|".join(data.index),
    types="|".join(types),
    fdr="\d+",


localrules:
    bed_chunks,
    dhs_null,
    model_input,
    make_model,
    merge_model_results,
    #make_fdr_d4,
    #make_fdr_peaks,
    fdr_bed,
    trackhub,
    coverage_tracks,
    fdr_tracks,
    sort_model,


# apply_model,


rule all:
    input:
        expand("results/{sm}/acc.model.results.bed.gz", sm=data.index),
        expand("results/{sm}/trackHub/hub.txt", sm=data.index),
        #expand("results/{sm}/fdr.peaks.d4", sm=data.index),
        expand("results/{sm}/fdr.peaks.and.coverages.bed.gz", sm=data.index),
        expand("results/{sm}/trackHub/bins/bin.{bin}.bed.bb", sm=data.index, bin=bins),


rule bed_chunks:
    input:
        ref=ref,
        fai=f"{ref}.fai",
    output:
        beds=temp(
            expand(
                "temp/{sm}/chunks/{chunk}.chunk.bed",
                chunk=chunks,
                allow_missing=True,
            )
        ),
    threads: 1
    conda:
        conda
    params:
        window_size=window_size,
        keep_chrs=keep_chrs,
    shell:
        """
        fibertools split \
          -g <(grep -w '{params.keep_chrs}' {input.fai} | sort -k1,1 -k2,2n -k3,3n -k4,4) \
          -o {output.beds}
        """


rule extract_and_split:
    input:
        bam=lambda wc: data.loc[wc.sm, "bam"],
        bed="temp/{sm}/chunks/{chunk}.chunk.bed",
    output:
        bed=temp("temp/{sm}/{chunk}.extract.all.bed.gz"),
    threads: 4
    resources:
        mem_mb=get_mem_mb,
    conda:
        conda
    shell:
        """
        samtools view \
          -F 2308 -b -M \
          -L {input.bed} \
          -@ {threads} {input.bam} \
          | ft -t {threads} extract --all - \
          | bgzip -@ {threads} \
          > {output.bed}
        """


rule dhs_null:
    input:
        fai=ancient(f"{ref}.fai"),
        dhs=dhs,
        exclude=excludes,
    output:
        bed="results/{sm}/dhs_with_null.bed.gz",
        exclude=temp("temp/{sm}/exclues.bed"),
    threads: 2
    conda:
        conda
    shell:
        """
        less {input.exclude} | cut -f 1-3 | bedtools sort -i - | bedtools merge -i - > {output.exclude}
        (
            bedtools intersect -v -a {input.dhs} -b {output.exclude} \
                | cut -f 1-3 | sed 's/$/\tDHS/g'; 
            bedtools shuffle -noOverlapping \
                -excl <( less {input.dhs} {output.exclude} | cut -f 1-3 | bedtools sort -i - | bedtools merge -i -) \
                -i <(bedtools intersect -v -a {input.dhs} -b {output.exclude}) \
                -g {input.fai} |
                cut -f 1-3 | sed 's/$/\tNULL/g' 
        ) |
            sort -k 1,1 -k2,2n --parallel={threads} -S 5G |
            grep -vw "chrM" |
            grep -vw "chrY" |
            grep -vw "chrEBV" |
            grep -vw "chrX" |
            bgzip -@ {threads} > {output.bed}
        """

rule filter_model_input_by_coverage:
    input:
        bed=rules.dhs_null.output.bed,
        bam=lambda wc: data.loc[wc.sm, "bam"],
    output:
        #tmp=temp("results/{sm}/dhs_with_null_with_cov.bed"),
        bed="results/{sm}/dhs_with_null_cov_filtered.bed",
    threads: 2
    conda:
        conda
    shell:
        """
        median=$(samtools depth {input.bam} -r chr1:30000000-40000000 | datamash median 3)
        min=$(echo "$median" | awk '{{print $1-3*sqrt($1)}}')
        max=$(echo "$median" | awk '{{print $1+3*sqrt($1)}}')
        echo $median $min $max 
        bedtools coverage -a {input.bed} -b {input.bam} -sorted \
            | awk -v min="$min" -v max="$max" '$5 > min && $5 < max' \
            | cut -f 1-4 \
            > {output.bed}
        """


rule model_input:
    input:
        bam=lambda wc: data.loc[wc.sm, "bam"],
        dhs=rules.filter_model_input_by_coverage.output.bed,
    output:
        bed=temp("temp/{sm}/small.extract.all.bed.gz"),
    threads: 16
    params:
        n=500_000,
    conda:
        conda
    shell:
        """ 
        (samtools view -F 2308 -u -M -L {input.dhs} -@ {threads} {input.bam} \
          | samtools view -@ {threads} -s 0.05 -u \
          | ft -t {threads} extract --all - \
          | bgzip -@ {threads} \
          > {output.bed} ) || echo "random head error"
        """


rule make_model:
    input:
        bed=rules.model_input.output.bed,
        dhs=rules.dhs_null.output.bed,
    output:
        model=protected("results/{sm}/model.dat"),
    benchmark:
        "benchmarks/{sm}/make_model.tsv"
    threads: 60
    conda:
        conda
    params:
        n=100_000,
    shell:
        """
        fibertools -t {threads} -v model \
            -n {params.n} \
            -o {output.model} \
            --dhs {input.dhs} \
            {input.bed} 
        """


def get_model(wc):
    if "model" in config:
        return ancient(config["model"])
    return ancient(rules.make_model.output.model)


rule apply_model:
    input:
        bed=rules.extract_and_split.output.bed,
        dhs=rules.dhs_null.output.bed,
        model=get_model,
    output:
        bed=temp("temp/{sm}/chunks/{chunk}.bed"),
    benchmark:
        "benchmarks/{sm}/chunks/apply_model_{chunk}.tsv"
    threads: 1
    resources:
        mem_mb=get_mem_mb,
    conda:
        conda
    shell:
        """
        fibertools -v model -m {input.model} -o {output.bed} {input.bed}
        """


rule sort_model:
    input:
        bed=rules.apply_model.output.bed,
    output:
        bed=temp("temp/{sm}/chunks/{chunk}.sorted.bed"),
    threads: 4
    conda:
        conda
    resources:
        mem_mb=get_mem_mb,
    shell:
        """
        sort --parallel={threads} -S 2G -k1,1 -k2,2n -k3,3n -k4,4 {input.bed} -o {output.bed}
        """


rule merge_model_results:
    input:
        beds=expand(rules.sort_model.output.bed, chunk=chunks, allow_missing=True),
    output:
        bed="results/{sm}/acc.model.results.bed.gz",
    benchmark:
        "benchmarks/{sm}/merge_model_results.tsv"
    threads: 8
    conda:
        conda
    params:
        n_chunks=n_chunks + 10,
    shell:
        """
        LC_ALL=C sort \
            --parallel={threads} \
            --batch-size={params.n_chunks} \
            -k1,1 -k2,2n -k3,3n -k4,4 -m -u \
            {input.beds} \
          | bgzip -@ {threads} \
          > {output.bed}
        """


rule index_model_results:
    input:
        bed=rules.merge_model_results.output.bed,
    output:
        tbi=rules.merge_model_results.output.bed + ".tbi",
    conda:
        conda
    shell:
        """
        tabix -p bed {input.bed}
        """


def get_load(wc):
    if "all" in wc.sm:
        return 100
    return 50


fake_fdr_bed = """
"""


rule make_fdr_d4:
    input:
        fai=ancient(f"{ref}.fai"),
        bed=rules.merge_model_results.output.bed,
        tbi=rules.index_model_results.output.tbi,
    output:
        d4=temp("temp/{sm}/{chrom}.fdr.coverages.d4"),
        bed=temp("temp/{sm}/{chrom}.fdr.coverages.bed"),
    benchmark:
        "benchmarks/{sm}/{chrom}.fdr.d4.tsv"
    threads: 4
    resources:
        mem_mb=get_mem_mb,
    conda:
        conda
    shell:
        """
        printf "{wildcards.chrom}\t0\t1\tfake\t100\t+\t0\t1\t230,230,230\t1.0\n" > {output.bed}
        printf "{wildcards.chrom}\t0\t1\tfake\t100\t+\t0\t1\t147,112,219\t1.0\n" >> {output.bed}
        printf "{wildcards.chrom}\t0\t1\tfake\t3\t+\t0\t1\t255,0,0\t0.03\n" >> {output.bed}
        tabix {input.bed} {wildcards.chrom} >> {output.bed}
        
        fibertools -v bed2d4 \
            --chromosome {wildcards.chrom} \
            -g {input.fai} \
            -c score \
            {output.bed} {output.d4}
        """


rule make_fdr_peaks:
    input:
        fai=ancient(f"{ref}.fai"),
        d4=rules.make_fdr_d4.output.d4,
    output:
        d4="temp/{sm}/{chrom}.fdr.peaks.d4",
    benchmark:
        "benchmarks/{sm}/{chrom}.fdr.peaks.tsv"
    threads: 4
    resources:
        mem_mb=get_mem_mb,
    conda:
        conda
    shell:
        """
        fibertools -v bed2d4 \
            --chromosome {wildcards.chrom} \
            -g {input.fai} \
            -c score \
            -q {input.d4} {output.d4}
        """


rule fdr_bed:
    input:
        peaks=rules.make_fdr_peaks.output.d4,
    output:
        bed="results/{sm}/chromosomes/{chrom}.fdr.peaks.and.coverages.bed.gz",
    threads: 4
    resources:
        mem_mb=get_mem_mb,
    conda:
        conda
    shell:
        """
        d4tools view {input.peaks} {wildcards.chrom} \
          | bgzip -@ {threads} \
        > {output.bed}
        #sort --parallel={threads} -S 2G -k1,1 -k2,2n -k3,3n -k4,4 
        """


rule chromosome_coverage_tracks:
    input:
        bed=rules.fdr_bed.output.bed,
    output:
        bed=temp("temp/{sm}/trackHub/bw/{chrom}.{types}.cov.bed"),
    threads: 4
    params:
        col=lambda wc: types_to_col[wc.types],
    conda:
        conda
    resources:
        mem_mb=get_mem_mb,
    shell:
        """
        bgzip -cd -@ {threads} {input.bed} | cut -f 1,2,3,{params.col} > {output.bed}
        """


rule coverage_tracks:
    input:
        beds=expand(
            rules.chromosome_coverage_tracks.output.bed,
            chrom=chroms,
            allow_missing=True,
        ),
        fai=ancient(f"{ref}.fai"),
    output:
        bed=temp("temp/{sm}/trackHub/bw/{types}.cov.bed"),
        bw="results/{sm}/trackHub/bw/{types}.bw",
    threads: 4
    conda:
        conda
    resources:
        mem_mb=get_mem_mb,
    shell:
        """
        cat {input.beds} > {output.bed}
        bedGraphToBigWig {output.bed} {input.fai} {output.bw}
        """


rule merged_fdr_track:
    input:
        beds=expand(
            rules.fdr_bed.output.bed,
            chrom=chroms,
            allow_missing=True,
        ),
        fai=ancient(f"{ref}.fai"),
    output:
        bed="results/{sm}/fdr.peaks.and.coverages.bed.gz",
        tbi="results/{sm}/fdr.peaks.and.coverages.bed.gz.tbi",
    threads: 4
    conda:
        conda
    resources:
        mem_mb=get_mem_mb,
    shell:
        """
        cat {input.beds} > {output.bed}
        tabix -p bed {output.bed}
        """


rule chromosome_fdr_tracks:
    input:
        bed=rules.fdr_bed.output.bed,
    output:
        bed=temp("temp/{sm}/trackHub/bw/{chrom}.fdr.{fdr}.bed"),
    threads: 4
    conda:
        conda
    resources:
        mem_mb=get_mem_mb,
    shell:
        """
        bgzip -cd -@ {threads} {input.bed} | cut -f 1-4 | awk '$4 > {wildcards.fdr}' > {output.bed}
        """


rule fdr_tracks:
    input:
        beds=expand(
            rules.chromosome_fdr_tracks.output.bed, chrom=chroms, allow_missing=True
        ),
        fai=ancient(f"{ref}.fai"),
    output:
        bw="results/{sm}/trackHub/bw/fdr.{fdr}.bw",
        bed=temp("temp/{sm}/trackHub/bw/fdr.{fdr}.bed"),
    threads: 4
    conda:
        conda
    resources:
        mem_mb=get_mem_mb,
    shell:
        """
        head -n 1 {input.fai} | awk '{{print $1"\t0\t1\t0"}}' > {output.bed}
        cat {input.beds} >> {output.bed}
        bedGraphToBigWig {output.bed} {input.fai} {output.bw}
        """


rule trackhub:
    input:
        bed=rules.merge_model_results.output.bed,
        fai=ancient(f"{ref}.fai"),
        bw=expand(rules.fdr_tracks.output.bw, fdr=[90, 100], allow_missing=True),
        fdr=expand(rules.coverage_tracks.output.bw, types="fdr", allow_missing=True),
        acc=expand(rules.coverage_tracks.output.bw, types="acc", allow_missing=True),
        link=expand(rules.coverage_tracks.output.bw, types="link", allow_missing=True),
        nuc=expand(rules.coverage_tracks.output.bw, types="nuc", allow_missing=True),
    output:
        hub="results/{sm}/trackHub/hub.txt",
    benchmark:
        "benchmarks/{sm}/trackhub.tsv"
    resources:
        load=get_load,
    threads: 4
    conda:
        conda
    params:
        ref=ref_name,
    shell:
        """
        fibertools -v trackhub \
          -r {params.ref} \
          --sample {wildcards.sm} \
          -t results/{wildcards.sm}/trackHub \
          {input.bed} {input.fai} \
          --bw {input.acc} {input.link} {input.nuc} {input.bw} {input.fdr}
        """


rule binned_fdr_calls:
    input:
        bed=rules.merge_model_results.output.bed,
        tbi=rules.index_model_results.output.tbi,
    output:
        beds=temp(
            expand("temp/{sm}/{chrom}.bin.{bin}.bed", bin=bins, allow_missing=True)
        ),
    benchmark:
        "benchmarks/{sm}/{chrom}.fdr.d4.tsv"
    threads: 4
    resources:
        mem_mb=get_mem_mb,
    conda:
        conda
    shell:
        """
        ((zcat {input.bed} | head -n 1) || true; tabix {input.bed} {wildcards.chrom}) \
            | fibertools -v bin - --outs {output.beds}
        """


rule merge_binned_fdr_calls:
    input:
        beds=expand("temp/{sm}/{chrom}.bin.{bin}.bed", chrom=chroms, allow_missing=True),
        fai=f"{ref}.fai",
    output:
        bed=temp("temp/{sm}/chromosomes/{bin}.bed"),
        bb="results/{sm}/trackHub/bins/bin.{bin}.bed.bb",
    threads: 1
    resources:
        mem_mb=get_mem_mb,
    conda:
        conda
    shell:
        """
        cat {input.beds} > {output.bed}
        bedToBigBed {output.bed} {input.fai} {output.bb}
        """
