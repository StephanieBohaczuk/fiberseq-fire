import pandas as pd
import sys
import os

ref = config["ref"]
ref_name = config["ref_name"]
n_chunks = config["n_chunks"]
chunks = [f"{i+1}-of-{n_chunks}" for i in range(n_chunks)]
excludes = config["excludes"]
dhs = config["dhs"]
max_t = config.get("max_t", 4)
keep_chrs = config.get("keep_chromosomes", ".*")


fai = pd.read_csv(f"{ref}.fai", sep="\t", names=["chr", "length", "x", "y", "z"])
chroms = [chrom for chrom in fai["chr"] if "_" not in chrom]
genome_len = fai["length"].sum()
window_size = int(genome_len / n_chunks) + 1
conda = "fiberseq-smk"

data = pd.read_csv(config["manifest"], sep="\t", comment="#").set_index("sample")

print(data.index)

types = ["fdr", "acc", "link", "nuc"]
types_to_col = {"fdr": 4, "acc": 5, "link": 6, "nuc": 7}


wildcard_constraints:
    chrom="|".join(chroms),
    call="|".join(["msp", "m6a"]),
    chunk="|".join(chunks),
    sm="|".join(data.index),
    types="|".join(types),


localrules:
    bed_chunks,
    dhs_null,
    model_input,
    make_model,
    merge_model_results,
    make_fdr_d4,
    make_fdr_peaks,
    fdr_bed,
    trackhub,
    coverage_tracks,
    fdr_tracks,
    sort_model,


# apply_model,


rule all:
    input:
        expand("results/{sm}/acc.model.results.bed.gz", sm=data.index),
        expand("results/{sm}/trackHub/hub.txt", sm=data.index),
        expand("results/{sm}/fdr.peaks.d4", sm=data.index),
        expand("results/{sm}/fdr.peaks.and.coverages.bed.gz", sm=data.index),


rule bed_chunks:
    input:
        ref=ref,
        fai=f"{ref}.fai",
    output:
        beds=temp(
            expand(
                "temp/{sm}/chunks/{chunk}.chunk.bed",
                chunk=chunks,
                allow_missing=True,
            )
        ),
    threads: 1
    conda:
        conda
    params:
        window_size=window_size,
        keep_chrs=keep_chrs,
    shell:
        """
        fibertools split \
          -g <(grep -w '{params.keep_chrs}' {input.fai}) \
          -o {output.beds}
        """


rule extract_and_split:
    input:
        bam=lambda wc: data.loc[wc.sm, "bam"],
        bed="temp/{sm}/chunks/{chunk}.chunk.bed",
    output:
        bed=temp("temp/{sm}/{chunk}.extract.all.bed.gz"),
    threads: 4
    conda:
        conda
    shell:
        """
        samtools view \
          -F 2308 -b -M \
          -L {input.bed} \
          -@ {threads} {input.bam} \
          | ft -t {threads} extract --all - \
          | bgzip -@ {threads} \
          > {output.bed}
        """


rule dhs_null:
    input:
        fai=ancient(f"{ref}.fai"),
        dhs=dhs,
        exclude=excludes,
    output:
        bed="results/{sm}/dhs_with_null.bed.gz",
        exclude=temp("temp/{sm}/exclues.bed"),
    threads: 2
    conda:
        conda
    shell:
        """
        less {input.exclude} | cut -f 1-3 | bedtools sort -i - | bedtools merge -i - > {output.exclude}
        (
            bedtools intersect -v -a {input.dhs} -b {output.exclude} \
                | cut -f 1-3 | sed 's/$/\tDHS/g'; 
            bedtools shuffle -noOverlapping \
                -excl <( less {input.dhs} {output.exclude} | cut -f 1-3 | bedtools sort -i - | bedtools merge -i -) \
                -i <(bedtools intersect -v -a {input.dhs} -b {output.exclude}) \
                -g {input.fai} |
                cut -f 1-3 | sed 's/$/\tNULL/g' 
        ) |
            sort -k 1,1 -k2,2n --parallel={threads} -S 5G |
            bgzip -@ {threads} > {output.bed}
        """


rule model_input:
    input:
        bam=lambda wc: data.loc[wc.sm, "bam"],
        dhs=rules.dhs_null.output.bed,
    output:
        bed=temp("temp/{sm}/small.extract.all.bed.gz"),
    threads: 8
    params:
        n=500_000,
    conda:
        conda
    shell:
        """ 
        (samtools view \
          -F 2308 -s 0.2 -h -M -L {input.dhs} -@ {threads} {input.bam} \
          | grep -v "^@PG"  \
          | head -n {params.n} \
          | samtools view -b \
          | ft -t {threads} extract --all - \
          | bgzip -@ {threads} \
          > {output.bed} ) || echo "random head error"
        """


rule make_model:
    input:
        bed=rules.model_input.output.bed,
        dhs=rules.dhs_null.output.bed,
    output:
        model=protected("results/{sm}/model.dat"),
    benchmark:
        "benchmarks/{sm}/make_model.tsv"
    threads: 60
    conda:
        conda
    shell:
        """
        fibertools -t {threads} -v model --dhs {input.dhs} -o {output.model} {input.bed}
        """


def get_model(wc):
    if "model" in config:
        return ancient(config["model"])
    return ancient(rules.make_model.output.model)


rule apply_model:
    input:
        bed=rules.extract_and_split.output.bed,
        dhs=rules.dhs_null.output.bed,
        model=get_model,
    output:
        bed=temp("temp/{sm}/chunks/{chunk}.bed"),
    benchmark:
        "benchmarks/{sm}/chunks/apply_model_{chunk}.tsv"
    threads: 1
    resources:
        mem_mb=1024 * 32,
    conda:
        conda
    shell:
        """
        fibertools -v model -m {input.model} -o {output.bed} {input.bed}
        """


rule sort_model:
    input:
        bed=rules.apply_model.output.bed,
    output:
        bed=temp("temp/{sm}/chunks/{chunk}.sorted.bed"),
    threads: 4
    conda:
        conda
    resources:
        mem_mb=1024 * 32,
    shell:
        """
        sort --parallel={threads} -S 2G -k1,1 -k2,2n -k3,3n -k4,4 {input.bed} -o {output.bed}
        """


rule merge_model_results:
    input:
        beds=expand(rules.sort_model.output.bed, chunk=chunks, allow_missing=True),
    output:
        bed="results/{sm}/acc.model.results.bed.gz",
    benchmark:
        "benchmarks/{sm}/merge_model_results.tsv"
    threads: 8
    conda:
        conda
    params:
        n_chunks=n_chunks + 1,
    shell:
        """
        sort --parallel={threads} \
          --batch-size={params.n_chunks} -k1,1 -k2,2n -k3,3n -k4,4 -S 1G -u -m {input.beds} \
          | bgzip -@ {threads} \
          > {output.bed}
        tabix -p bed {output.bed}
        """


rule index_model_results:
    input:
        bed=rules.merge_model_results.output.bed,
    output:
        tbi=rules.merge_model_results.output.bed + ".tbi",
    conda:
        conda
    shell:
        """
        tabix -p bed {input.bed}
        """


def get_load(wc):
    if "all" in wc.sm:
        return 1000
    return 500


rule make_fdr_d4:
    input:
        fai=ancient(f"{ref}.fai"),
        bed=rules.merge_model_results.output.bed,
        tbi=rules.index_model_results.output.tbi,
    output:
        d4=temp("temp/{sm}/{chrom}.fdr.coverages.d4"),
        bed=temp("temp/{sm}/{chrom}.fdr.coverages.bed"),
    benchmark:
        "benchmarks/{sm}/{chrom}.fdr.d4.tsv"
    threads: 16
    resources:
        load=get_load,
    conda:
        conda
    shell:
        """
        tabix {input.bed} {wildcards.chrom} > {output.bed}
        
        fibertools -v bed2d4 \
            -g {input.fai} \
            -c score \
            {output.bed} {output.d4}
        """


rule make_fdr_peaks:
    input:
        fai=ancient(f"{ref}.fai"),
        d4=rules.make_fdr_d4.output.d4,
    output:
        d4="temp/{sm}/{chrom}.fdr.peaks.d4",
    benchmark:
        "benchmarks/{sm}/{chrom}.fdr.peaks.tsv"
    threads: 16
    resources:
        load=get_load,
    conda:
        conda
    shell:
        """
        fibertools -v bed2d4 \
          -g {input.fai} \
          -c score \
          -q {input.d4} {output.d4}
        """


rule fdr_bed:
    input:
        peaks=rules.make_fdr_peaks.output.d4,
    output:
        bed="results/{sm}/chromosomes/{chrom}.fdr.peaks.and.coverages.bed.gz",
    threads: 8
    resources:
        mem_mb=1024 * 32,
    conda:
        conda
    shell:
        """
        d4tools view {input.peaks} \
          | sort --parallel={threads} -S 2G -k1,1 -k2,2n -k3,3n -k4,4 \
          | bgzip -@ {threads} \
        > {output.bed}
        """


rule chromosome_coverage_tracks:
    input:
        bed=rules.fdr_bed.output.bed,
    output:
        bed=temp("temp/{sm}/trackHub/bw/{chrom}.{types}.cov.bed"),
    threads: 4
    params:
        col=lambda wc: types_to_col[wc.types],
    conda:
        conda
    resources:
        mem_mb=1024 * 32,
    shell:
        """
        bgzip -cd -@ {threads} {input.bed} | cut -f 1,2,3,{params.col} > {output.bed}
        """


rule coverage_tracks:
    input:
        beds=expand(
            rules.chromosome_coverage_tracks.output.bed,
            chrom=chroms,
            allow_missing=True,
        ),
        fai=ancient(f"{ref}.fai"),
    output:
        bed=temp("temp/{sm}/trackHub/bw/{types}.cov.bed"),
        bw="results/{sm}/trackHub/bw/{types}.bw",
    threads: 4
    conda:
        conda
    resources:
        mem_mb=1024 * 32,
    shell:
        """
        cat {input.beds} > {output.bed}
        bedGraphToBigWig {output.bed} {input.fai} {output.bw}
        """


rule chromosome_fdr_tracks:
    input:
        bed=rules.fdr_bed.output.bed,
    output:
        bed=temp("temp/{sm}/trackHub/bw/{chrom}.fdr.{fdr}.bed"),
    threads: 4
    conda:
        conda
    resources:
        mem_mb=1024 * 32,
    shell:
        """
        bgzip -cd -@ {threads} {input.bed} | cut -f 1-4 | awk '$4 > {wildcards.fdr}' > {output.bed}
        """


rule fdr_tracks:
    input:
        bed=expand(rules.chromosome_fdr_bed.output.bed, chrom=chroms, allow_missing=True),
        fai=ancient(f"{ref}.fai"),
    output:
        bw="results/{sm}/trackHub/bw/fdr.{fdr}.bw",
        bed=temp("temp/{sm}/trackHub/bw/fdr.{fdr}.bed"),
    threads: 4
    conda:
        conda
    resources:
        mem_mb=1024 * 32,
    shell:
        """
        head -n 1 {input.fai} | awk '{{print $1"\t0\t1\t0"}}' > {output.bed}
        cat {input.beds} >> {output.bed}
        bedGraphToBigWig {output.bed} {input.fai} {output.bw}
        """


rule trackhub:
    input:
        bed=rules.merge_model_results.output.bed,
        fai=ancient(f"{ref}.fai"),
        bw=expand(rules.fdr_tracks.output.bw, fdr=[90, 100], allow_missing=True),
        fdr=expand(rules.coverage_tracks.output.bw, types="fdr", allow_missing=True),
        acc=expand(rules.coverage_tracks.output.bw, types="acc", allow_missing=True),
        link=expand(rules.coverage_tracks.output.bw, types="link", allow_missing=True),
        nuc=expand(rules.coverage_tracks.output.bw, types="nuc", allow_missing=True),
    output:
        hub="results/{sm}/trackHub/hub.txt",
    benchmark:
        "benchmarks/{sm}/trackhub.tsv"
    resources:
        load=get_load,
    threads: 16
    conda:
        conda
    params:
        ref=ref_name,
    shell:
        """
        fibertools -v trackhub \
          -r {params.ref} \
          --sample {wildcards.sm} \
          -t results/{wildcards.sm}/trackHub \
          {input.bed} {input.fai} \
          --bw {input.acc} {input.link} {input.nuc} {input.bw} {input.fdr}
        """
