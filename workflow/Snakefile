import pandas as pd
import sys
import os

ref = config["ref"]
ref_name = config["ref_name"]
n_chunks = config["n_chunks"]
chunks = [f"{i+1}-of-{n_chunks}" for i in range(n_chunks)]
excludes = config["excludes"]
dhs = config["dhs"]
max_t = config.get("max_t", 4)
max_bins = config.get("bins", 75)
keep_chrs = config.get("keep_chromosomes", ".*")


fai = pd.read_csv(f"{ref}.fai", sep="\t", names=["chr", "length", "x", "y", "z"])
chroms = sorted([chrom for chrom in fai["chr"] if "_" not in chrom])
genome_len = fai["length"].sum()
window_size = int(genome_len / n_chunks) + 1
conda = "fiberseq-smk"
bins = list(range(1, max_bins + 1))
data = pd.read_csv(config["manifest"], sep="\t", comment="#").set_index("sample")
haps = ["all", "hap1", "hap2", "unk"]
not_all = ["hap1", "hap2", "unk"]
not_unk = ["all", "hap1", "hap2"]
print(data.index)

types = ["fdr", "acc", "link", "nuc"]
types_to_col = {"fdr": 4, "acc": 5, "link": 6, "nuc": 7}


def get_mem_mb(wildcards, attempt):
    if attempt < 3:
        return attempt * 1024 * 32
    return attempt * 1024 * 48


def get_large_mem_mb(wildcards, attempt):
    return attempt * 1024 * 64


def get_load(wc):
    if "all" in wc.sm:
        return 100
    return 50


wildcard_constraints:
    chrom="|".join(chroms),
    call="|".join(["msp", "m6a"]),
    chunk="|".join(chunks),
    sm="|".join(data.index),
    types="|".join(types),
    fdr="\d+",
    hp="|".join(haps),


include: "rules/train-model.smk"
include: "rules/apply-model.smk"
include: "rules/track-hub.smk"


localrules:
    bed_chunks,
    dhs_null,
    model_input,
    make_model,
    merge_model_results,
    model_bam,
    fdr_bed,
    trackhub,
    coverage_tracks,
    fdr_tracks,
    sort_model,
    average_coverage,
    clustering_vs_null,
    percent_in_clusters,


rule all:
    input:
        expand("results/{sm}/{hp}/acc.model.results.bed.gz", sm=data.index, hp=haps),
        expand("results/{sm}/trackHub/hub.txt", sm=data.index),
        expand(
            "results/{sm}/{hp}/fdr.peaks.and.coverages.bed.gz", sm=data.index, hp=haps
        ),
        expand(
            "results/{sm}/trackHub/bins/{hp}.bin.{bin}.bed.bb",
            sm=data.index,
            bin=bins,
            hp=not_all,
        ),
        expand(rules.merge_peak_calls.output.bed, sm=data.index, hp=not_unk),
        expand(rules.clustering_vs_null.output.bed, sm=data.index),
        expand(rules.percent_in_clusters.output, sm=data.index),
        expand(rules.n_peaks.output, sm=data.index),
